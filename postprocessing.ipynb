{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08f776ae-69ef-4257-9774-051d3db3a1ae",
      "metadata": {
        "id": "08f776ae-69ef-4257-9774-051d3db3a1ae"
      },
      "outputs": [],
      "source": [
        "!{sys.executable} -m pip install nltk spacy lxml\n",
        "!{sys.executable} -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e12b36a-07f1-4bd5-bdf1-41193f131df7",
      "metadata": {
        "id": "3e12b36a-07f1-4bd5-bdf1-41193f131df7"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import locale\n",
        "from collections import defaultdict\n",
        "import datetime\n",
        "\n",
        "locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n",
        "import datetime as dt\n",
        "\n",
        "# NLTK imports\n",
        "import nltk\n",
        "\n",
        "nltk.data.path.append('../nltk_data/')\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "from nltk import collocations\n",
        "from nltk.text import Text\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk import RegexpParser\n",
        "from nltk.tree import *\n",
        "\n",
        "# spaCy imports\n",
        "import spacy\n",
        "from spacy.symbols import nsubj, VERB\n",
        "\n",
        "from time import sleep\n",
        "from lxml import etree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61a1ae15-96eb-4f2f-a057-5cf05c847b31",
      "metadata": {
        "id": "61a1ae15-96eb-4f2f-a057-5cf05c847b31"
      },
      "outputs": [],
      "source": [
        "def lemmatize_verb(verb):\n",
        "    # Lemmatize the verb using spaCy\n",
        "    doc = nlp(verb)\n",
        "    return doc[0].lemma_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca22a8bd-587b-4f52-a19a-984895a8e525",
      "metadata": {
        "id": "ca22a8bd-587b-4f52-a19a-984895a8e525"
      },
      "outputs": [],
      "source": [
        "with open('pal_affiliations.json', 'r') as file:\n",
        "    PALESTINE_MEMBER_AFFILIATIONS = json.load(file)\n",
        "\n",
        "with open('israel_affiliations.json', 'r') as file:\n",
        "    ISRAEL_MEMBER_AFFILIATIONS = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d809d8-fcf2-41ef-bdbb-3f139172cd06",
      "metadata": {
        "id": "c8d809d8-fcf2-41ef-bdbb-3f139172cd06"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Choosing a dataset\n",
        "sample_size = 1\n",
        "\n",
        "# Defining the dataset path\n",
        "results_prefix = \"./results\"\n",
        "\n",
        "input_files = os.listdir(results_prefix + '/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a22c061-d906-4cd9-999d-2f229af638bf",
      "metadata": {
        "id": "6a22c061-d906-4cd9-999d-2f229af638bf"
      },
      "outputs": [],
      "source": [
        "def follow_compound(dep_idx, dependencies_by_governor):\n",
        "    # Follow compound chain and return descriptors of a dependency\n",
        "    visited = set([dep_idx])\n",
        "    current_idx = dep_idx\n",
        "    descriptors = set()\n",
        "\n",
        "    found = True\n",
        "    while found:\n",
        "        found = False\n",
        "        for dep in dependencies_by_governor[current_idx]:\n",
        "            if dep[\"dep\"] == \"compound:prt\" or dep[\"dep\"] == \"compound\":\n",
        "                current_idx = dep[\"dependent\"]\n",
        "                descriptors.add(dep[\"dependentGloss\"])\n",
        "                if current_idx not in visited:\n",
        "                    found = True\n",
        "                    visited.add(current_idx)\n",
        "                break\n",
        "    return descriptors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd89336b-bbdd-45ee-a46f-554ddc4ebf59",
      "metadata": {
        "id": "bd89336b-bbdd-45ee-a46f-554ddc4ebf59"
      },
      "outputs": [],
      "source": [
        "def investigate_subject(subject, dependencies_by_governor):\n",
        "    # Investigate all dependencies related to a subject to find as many descriptors as possible\n",
        "    # Present in a tiered list based on \"closeness\" to subject\n",
        "\n",
        "    verbose = False\n",
        "\n",
        "    subject_descriptors = [set(), set(), set(), 1]\n",
        "    subject_descriptors[0].add(subject[1])\n",
        "    # AMOD takes precedence over NMOD? takes precedence over ACL\n",
        "    for dep in dependencies_by_governor[subj_idx]:\n",
        "        if (dep[\"dep\"] == \"amod\"):\n",
        "            subject_descriptors[0].add(dep[\"dependentGloss\"])\n",
        "            if verbose:\n",
        "                print(dep[\"dep\"], dep[\"dependentGloss\"], \"\\n\")\n",
        "\n",
        "        if (dep[\"dep\"] == \"acl\" or dep[\"dep\"] == \"acl:relcl\"):\n",
        "            subject_descriptors[1].add(dep[\"dependentGloss\"])\n",
        "            # INVESTIGATE THE SUBJECT AND OBJECT OF DESCRIPTIVE CLAUSE\n",
        "            if verbose:\n",
        "                print(dep[\"dep\"], dep[\"dependentGloss\"])\n",
        "                print(dependencies_by_governor[dep[\"dependent\"]])\n",
        "                print()\n",
        "            for double_dep in dependencies_by_governor[dep[\"dependent\"]]:\n",
        "                # check nsubj and check obj\n",
        "                if (double_dep[\"dep\"] == \"nsubj\" or double_dep[\"dep\"] == \"nsubj:pass\" or double_dep[\n",
        "                    \"dep\"] == \"nsubj:outer\"\n",
        "                        or double_dep[\"dep\"] == \"csubj\" or double_dep[\"dep\"] == \"csubj:pass\" or double_dep[\n",
        "                            \"dep\"] == \"csubj:outer\"\n",
        "                        or double_dep[\"dep\"] == \"obj\"):\n",
        "                    subject_descriptors[2].add(double_dep[\"dependentGloss\"])\n",
        "\n",
        "        if (dep[\"dep\"] == \"nmod\" or dep[\"dep\"] == \"nmod:npmod\" or dep[\"dep\"] == \"nmod:tmod\" or dep[\n",
        "            \"dep\"] == \"nmod:poss\"):\n",
        "            subject_descriptors[1].add(dep[\"dependentGloss\"])\n",
        "            if verbose:\n",
        "                print(dep[\"dep\"], dep[\"dependentGloss\"], \"\\n\")\n",
        "\n",
        "        if (dep[\"dep\"] == \"advmod\"):\n",
        "            subject_descriptors[1].add(dep[\"dependentGloss\"])  # TODO: CHECK THIS\n",
        "            if verbose:\n",
        "                print(dep[\"dep\"], dep[\"dependentGloss\"], \"\\n\")\n",
        "\n",
        "        if (dep[\"dep\"] == \"appos\"):\n",
        "            subject_descriptors[0].add(dep[\"dependentGloss\"])  # TODO:CHECK THIS\n",
        "            # Look for adjectives for appos also\n",
        "            # CHECK COMPOUD AND AMOD\n",
        "            to_add = follow_compound(dep[\"dependent\"], dependencies_by_governor)\n",
        "            if verbose:\n",
        "                print(dep[\"dep\"], dep[\"dependentGloss\"], \"\\n\", to_add)\n",
        "            for thing in to_add:\n",
        "                subject_descriptors[0].add(thing)\n",
        "            for double_dep in dependencies_by_governor[dep[\"dependent\"]]:\n",
        "                if (double_dep[\"dep\"] == \"amod\"):\n",
        "                    subject_descriptors[2].add(double_dep[\"dependentGloss\"])\n",
        "\n",
        "        #         if (dep[\"dep\"] == \"ccomp\"):\n",
        "        #             print(dependencies_by_governor[dep[\"dependent\"]])\n",
        "        #             for double_dep in dependencies_by_governor[dep[\"dependent\"]]:\n",
        "        #                 if (double_dep[\"dep\"] == \"nsubj\" or double_dep[\"dep\"] == \"nsubj:pass\"\n",
        "        #                      or double_dep[\"dep\"] == \"csubj\" or double_dep[\"dep\"] == \"csubj:pass\"):\n",
        "        #                     subject_descriptors[2].add(double_dep[\"dependentGloss\"])\n",
        "\n",
        "        if (dep[\"dep\"] == \"nummod\"):\n",
        "            try:\n",
        "                subject_descriptors[3] = locale.atoi(dep[\"dependentGloss\"])\n",
        "            except:\n",
        "                subject_descriptors[3] = dep[\"dependentGloss\"]\n",
        "\n",
        "    return subject_descriptors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eb36f6d-ac66-448d-a552-e5df545c9e23",
      "metadata": {
        "id": "8eb36f6d-ac66-448d-a552-e5df545c9e23"
      },
      "outputs": [],
      "source": [
        "def extract_sentences(sentences):\n",
        "    # extract all sentences in an article\n",
        "    sentences_text = [None] * len(sentences)\n",
        "    for sentence in sentences:\n",
        "        sentence_index = sentence[\"index\"]\n",
        "\n",
        "        tokens = sentence[\"tokens\"]\n",
        "        sentence_text = \"\"\n",
        "        for token in tokens:\n",
        "            sentence_text += token[\"before\"] + token[\"word\"] + token[\"after\"]\n",
        "\n",
        "        sentences_text[sentence_index] = sentence_text\n",
        "    return sentences_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f259a4f-9fe1-4cf4-b2bf-61fe8b2e31f8",
      "metadata": {
        "id": "2f259a4f-9fe1-4cf4-b2bf-61fe8b2e31f8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "def refresh_screen():\n",
        "    clear_output()\n",
        "    sleep(0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32eb405-6dff-4a9e-bcd8-c57e4f5416e6",
      "metadata": {
        "tags": [],
        "id": "c32eb405-6dff-4a9e-bcd8-c57e4f5416e6"
      },
      "outputs": [],
      "source": [
        "# load data for annotation\n",
        "df = pd.read_csv('./data/summary_20240421_articles.csv')\n",
        "df['date'] = pd.to_datetime(df['date'], format='mixed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3539765-0fa2-4915-b6cf-477b7bb9324b",
      "metadata": {
        "id": "f3539765-0fa2-4915-b6cf-477b7bb9324b"
      },
      "outputs": [],
      "source": [
        "root_directory = './'\n",
        "\n",
        "# Define variables to track counts\n",
        "both_present_count = 0\n",
        "antisemitic_only_count = 0\n",
        "islamophobic_only_count = 0\n",
        "\n",
        "# Loop through each article\n",
        "for index, row in df.iterrows():\n",
        "\n",
        "    # Initialize variables to track presence of keywords\n",
        "    antisemitic_present = False\n",
        "    islamophobic_present = False\n",
        "\n",
        "    results_file = row['results_file']\n",
        "    article_file = row['article_file']\n",
        "\n",
        "    filename = root_directory + results_file\n",
        "\n",
        "    try:\n",
        "        with open(filename) as d:\n",
        "            data = json.load(d)\n",
        "    except FileNotFoundError:\n",
        "        print('FILE NOT FOUND')\n",
        "        continue\n",
        "\n",
        "    # Open original text block from preprocessed data file\n",
        "    original_filename = root_directory + article_file\n",
        "    f = open(original_filename, \"r\")\n",
        "    article_text = f.read()\n",
        "    f.close()\n",
        "\n",
        "\n",
        "    sentences = data[\"sentences\"]\n",
        "    text_all_sentences = extract_sentences(sentences)\n",
        "\n",
        "\n",
        "    # Process each sentence for keyword presence\n",
        "    for sentence in text_all_sentences:\n",
        "        sentence_text = sentence\n",
        "        # Check for presence of keywords\n",
        "        if \"antisemitic\" in sentence_text.lower() or \"antisemitism\" in sentence_text.lower() or \"anti-jew\" in sentence_text.lower() or \"anti-semitic\" in sentence_text.lower() or \"anti-semitism\" in sentence_text.lower():\n",
        "            antisemitic_present = True\n",
        "        if \"islamophobic\" in sentence_text.lower() or \"islamophobia\" in sentence_text.lower() or \"anti-muslim\" in sentence_text.lower() or \"anti-arab\" in sentence_text.lower():\n",
        "            islamophobic_present = True\n",
        "\n",
        "    # Update counts based on keyword presence\n",
        "    if antisemitic_present and islamophobic_present:\n",
        "        both_present_count += 1\n",
        "    elif antisemitic_present:\n",
        "        antisemitic_only_count += 1\n",
        "    elif islamophobic_present:\n",
        "        islamophobic_only_count += 1\n",
        "\n",
        "# Print or store the counts as needed\n",
        "print(\"Articles containing both 'antisemitic'/'antisemitism' and 'Islamophobic'/'Islamophobia':\", both_present_count)\n",
        "print(\"Articles containing 'antisemitic'/'antisemitism' but not 'Islamophobic'/'Islamophobia':\", antisemitic_only_count)\n",
        "print(\"Articles containing 'Islamophobic'/'Islamophobia' but not 'antisemitic'/'antisemitism':\", islamophobic_only_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4543e43-caa7-4847-925e-e06ca5bdb5ce",
      "metadata": {
        "id": "b4543e43-caa7-4847-925e-e06ca5bdb5ce"
      },
      "outputs": [],
      "source": [
        "def count_passive_objects(sentences, counts):\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence[\"tokens\"]\n",
        "        dependencies = sentence[\"basicDependencies\"]\n",
        "\n",
        "        is_passive = False\n",
        "        has_agent = False\n",
        "        root_verb = None\n",
        "\n",
        "        # Check if the sentence is passive and has no agent\n",
        "        for dep in dependencies:\n",
        "            if dep[\"dep\"] == \"nsubj:pass\" or dep[\"dep\"] == \"csubj:pass\":\n",
        "                is_passive = True\n",
        "            if (dep[\"dep\"] == \"nmod\" or dep[\"dep\"] == \"obl:agent\"):\n",
        "                has_agent = True\n",
        "            if dep[\"dep\"] == \"ROOT\":  # Extract the root verb\n",
        "                root_verb = dep.get(\"dependentGloss\")\n",
        "\n",
        "        # If the sentence is passive and has no agent, count occurrences of the target words as the subject\n",
        "        if is_passive:\n",
        "            for dep in dependencies:\n",
        "                if dep[\"dep\"] == \"nsubj\":\n",
        "                    subject_word = dep.get(\"dependentGloss\")\n",
        "                    if subject_word in ISRAEL_MEMBER_AFFILIATIONS:\n",
        "                        counts['israel']['passive_no_agent' if not has_agent else 'passive_with_agent'] += 1\n",
        "                    elif subject_word in PALESTINE_MEMBER_AFFILIATIONS:\n",
        "                        counts['palestine']['passive_no_agent' if not has_agent else 'passive_with_agent'] += 1\n",
        "\n",
        "        # Check if the sentence is active and count occurrences of the target words as the subject\n",
        "        else:\n",
        "            for dep in dependencies:\n",
        "                if dep[\"dep\"] == \"nsubj\":\n",
        "                    subject_word = dep.get(\"dependentGloss\")\n",
        "                    if subject_word in ISRAEL_MEMBER_AFFILIATIONS:\n",
        "                        counts['israel']['active'] += 1\n",
        "                    elif subject_word in PALESTINE_MEMBER_AFFILIATIONS:\n",
        "                        counts['palestine']['active'] += 1\n",
        "\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55cbcafb-6593-442e-85dd-0cc7d1f783d2",
      "metadata": {
        "id": "55cbcafb-6593-442e-85dd-0cc7d1f783d2"
      },
      "outputs": [],
      "source": [
        "root_directory = './'\n",
        "\n",
        "counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    results_file = row['results_file']\n",
        "    article_file = row['article_file']\n",
        "\n",
        "\n",
        "    # Open NLP-analyzed result\n",
        "    filename = root_directory + results_file\n",
        "    try:\n",
        "        with open(filename) as d:\n",
        "            data = json.load(d)\n",
        "    except FileNotFoundError:\n",
        "        print('FILE NOT FOUND')\n",
        "        continue\n",
        "\n",
        "    # Open original text block from preprocessed data file\n",
        "    original_filename = root_directory + article_file\n",
        "    f = open(original_filename, \"r\")\n",
        "    article_text = f.read()\n",
        "    f.close()\n",
        "\n",
        "    # Extract original date\n",
        "    title, date = row['title'], row['date'].date().strftime('%Y-%m-%d')\n",
        "\n",
        "    # Extract NLP results\n",
        "    sentences = data[\"sentences\"]\n",
        "    counts = count_passive_objects(sentences, counts)\n",
        "\n",
        "with open('patient_agent_counts.json', 'w') as json_file:\n",
        "    json.dump(counts, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d7fb91b-b2e4-48cf-8a06-4bac3d9e50f0",
      "metadata": {
        "id": "9d7fb91b-b2e4-48cf-8a06-4bac3d9e50f0"
      },
      "outputs": [],
      "source": [
        "counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce349b8-aa5c-4f88-b52f-e7f62743346d",
      "metadata": {
        "id": "8ce349b8-aa5c-4f88-b52f-e7f62743346d"
      },
      "outputs": [],
      "source": [
        "counts['israel']['passive_no_agent']/(counts['israel']['passive_no_agent'] + counts['israel']['passive_with_agent'] + counts['israel']['active'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "194e0586-01c9-48ce-9722-72072865d32d",
      "metadata": {
        "id": "194e0586-01c9-48ce-9722-72072865d32d"
      },
      "outputs": [],
      "source": [
        "counts['palestine']['passive_no_agent']/(counts['palestine']['passive_no_agent'] + counts['palestine']['passive_with_agent'] + counts['palestine']['active'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05534f74-e277-40e2-9d6d-76cc1bea5337",
      "metadata": {
        "id": "05534f74-e277-40e2-9d6d-76cc1bea5337"
      },
      "outputs": [],
      "source": [
        "def get_active_verbs_with_affiliation_subjects(sentences, verb_counts):\n",
        "    for sentence in sentences:\n",
        "        dependencies = sentence[\"basicDependencies\"]\n",
        "        is_active = False\n",
        "        subject_word = None\n",
        "\n",
        "        # Check if the sentence is active and has a subject from either affiliation\n",
        "        for dep in dependencies:\n",
        "            if dep[\"dep\"] == \"nsubj\" or dep[\"dep\"] == \"csubj\":  # Active subject dependencies\n",
        "                subject_word = dep.get(\"dependentGloss\")\n",
        "                if subject_word in ISRAEL_MEMBER_AFFILIATIONS:\n",
        "                    category = \"israel\"\n",
        "                    is_active = True\n",
        "                    break\n",
        "                elif subject_word in PALESTINE_MEMBER_AFFILIATIONS:\n",
        "                    category = \"palestine\"\n",
        "                    is_active = True\n",
        "                    break\n",
        "\n",
        "        # If the sentence is active with a subject from either affiliation, collect the verb\n",
        "        if is_active:\n",
        "            for dep in dependencies:\n",
        "                if dep[\"dep\"] == \"ROOT\":  # Find the root verb of the sentence\n",
        "                    verb = dep.get(\"dependentGloss\")\n",
        "                    verb_lemma = lemmatize_verb(verb)\n",
        "                    verb_counts[category][verb_lemma] = verb_counts[category].get(verb_lemma, 0) + 1\n",
        "\n",
        "    return verb_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fbe5e95-f78a-4644-b879-b7330a403e21",
      "metadata": {
        "id": "7fbe5e95-f78a-4644-b879-b7330a403e21"
      },
      "outputs": [],
      "source": [
        "def get_passive_verbs_with_affiliation_subjects(sentences, verb_counts):\n",
        "    for sentence in sentences:\n",
        "        dependencies = sentence[\"basicDependencies\"]\n",
        "        is_passive = False\n",
        "        subject_word = None\n",
        "\n",
        "        # Check if the sentence is passive and has a subject from either affiliation\n",
        "        for dep in dependencies:\n",
        "            if dep[\"dep\"] == \"nsubj:pass\" or dep[\"dep\"] == \"csubj:pass\":  # Passive subject dependencies\n",
        "                subject_word = dep.get(\"dependentGloss\")\n",
        "                if subject_word in ISRAEL_MEMBER_AFFILIATIONS:\n",
        "                    category = \"israel\"\n",
        "                    is_passive = True\n",
        "                    break\n",
        "                elif subject_word in PALESTINE_MEMBER_AFFILIATIONS:\n",
        "                    category = \"palestine\"\n",
        "                    is_passive = True\n",
        "                    break\n",
        "\n",
        "        # If the sentence is active with a subject from either affiliation, collect the verb\n",
        "        if is_passive:\n",
        "            for dep in dependencies:\n",
        "                if dep[\"dep\"] == \"ROOT\":  # Find the root verb of the sentence\n",
        "                    verb = dep.get(\"dependentGloss\")\n",
        "                    verb_lemma = lemmatize_verb(verb)\n",
        "                    verb_counts[category][verb_lemma] = verb_counts[category].get(verb_lemma, 0) + 1\n",
        "\n",
        "    return verb_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a227d2ce-74b8-40f9-a3eb-8794a6e22fb2",
      "metadata": {
        "id": "a227d2ce-74b8-40f9-a3eb-8794a6e22fb2"
      },
      "outputs": [],
      "source": [
        "root_directory = './'\n",
        "\n",
        "verb_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    results_file = row['results_file']\n",
        "    article_file = row['article_file']\n",
        "\n",
        "\n",
        "    # Open NLP-analyzed result\n",
        "    filename = root_directory + results_file\n",
        "    try:\n",
        "        with open(filename) as d:\n",
        "            data = json.load(d)\n",
        "    except FileNotFoundError:\n",
        "        print('FILE NOT FOUND')\n",
        "        continue\n",
        "\n",
        "    # Open original text block from preprocessed data file\n",
        "    original_filename = root_directory + article_file\n",
        "    f = open(original_filename, \"r\")\n",
        "    article_text = f.read()\n",
        "    f.close()\n",
        "\n",
        "    # Extract original date\n",
        "    title, date = row['title'], row['date'].date().strftime('%Y-%m-%d')\n",
        "\n",
        "    # Extract NLP results\n",
        "    sentences = data[\"sentences\"]\n",
        "    verb_counts = get_passive_verbs_with_affiliation_subjects(sentences, verb_counts)\n",
        "\n",
        "with open('passive_verb_counts.json', 'w') as json_file:\n",
        "    json.dump(verb_counts, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86581be1-1f89-443d-a2f3-02a9e829ede3",
      "metadata": {
        "id": "86581be1-1f89-443d-a2f3-02a9e829ede3"
      },
      "outputs": [],
      "source": [
        "root_directory = './'\n",
        "\n",
        "verb_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    results_file = row['results_file']\n",
        "    article_file = row['article_file']\n",
        "\n",
        "\n",
        "    # Open NLP-analyzed result\n",
        "    filename = root_directory + results_file\n",
        "    try:\n",
        "        with open(filename) as d:\n",
        "            data = json.load(d)\n",
        "    except FileNotFoundError:\n",
        "        print('FILE NOT FOUND')\n",
        "        continue\n",
        "\n",
        "    # Open original text block from preprocessed data file\n",
        "    original_filename = root_directory + article_file\n",
        "    f = open(original_filename, \"r\")\n",
        "    article_text = f.read()\n",
        "    f.close()\n",
        "\n",
        "    # Extract original date\n",
        "    title, date = row['title'], row['date'].date().strftime('%Y-%m-%d')\n",
        "\n",
        "    # Extract NLP results\n",
        "    sentences = data[\"sentences\"]\n",
        "    verb_counts = get_active_verbs_with_affiliation_subjects(sentences, verb_counts)\n",
        "\n",
        "with open('active_verb_counts.json', 'w') as json_file:\n",
        "    json.dump(verb_counts, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "511b0253-3838-417d-a628-c122bd08ee1c",
      "metadata": {
        "id": "511b0253-3838-417d-a628-c122bd08ee1c"
      },
      "outputs": [],
      "source": [
        "verb_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1ddfd0-d2ec-48a4-ab27-4da8bf7c54c4",
      "metadata": {
        "id": "0c1ddfd0-d2ec-48a4-ab27-4da8bf7c54c4"
      },
      "outputs": [],
      "source": [
        "root_directory = './'\n",
        "\n",
        "verb_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    results_file = row['results_file']\n",
        "    article_file = row['article_file']\n",
        "\n",
        "\n",
        "    # Open NLP-analyzed result\n",
        "    filename = root_directory + results_file\n",
        "    try:\n",
        "        with open(filename) as d:\n",
        "            data = json.load(d)\n",
        "    except FileNotFoundError:\n",
        "        print('FILE NOT FOUND')\n",
        "        continue\n",
        "\n",
        "    # Open original text block from preprocessed data file\n",
        "    original_filename = root_directory + article_file\n",
        "    f = open(original_filename, \"r\")\n",
        "    article_text = f.read()\n",
        "    f.close()\n",
        "\n",
        "    # Extract original date\n",
        "    title, date = row['title'], row['date'].date().strftime('%Y-%m-%d')\n",
        "\n",
        "    # Extract NLP results\n",
        "    sentences = data[\"sentences\"]\n",
        "    verb_counts = get_active_verbs_with_affiliation_subjects(sentences, verb_counts)\n",
        "\n",
        "with open('active_verb_counts.json', 'w') as json_file:\n",
        "    json.dump(counts, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a0089e1-f574-4024-9424-3c6dfa342e1f",
      "metadata": {
        "id": "0a0089e1-f574-4024-9424-3c6dfa342e1f"
      },
      "outputs": [],
      "source": [
        "pal_affiliates = PALESTINE_MEMBER_AFFILIATIONS.copy()\n",
        "pal_affiliates.remove(\"Hamas\")\n",
        "\n",
        "PALESTINE_IDENTIFIERS = [\"Palestine\", \"Palestinian\", \"Palestinians\", \"Gaza\", \"Gazan\", \"Gazans\"]\n",
        "ISRAEL_IDENTIFIERS = [\"Israel\", \"Israeli\", \"Israelis\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ceb558-7fce-46c3-82a4-8856cf065b01",
      "metadata": {
        "id": "52ceb558-7fce-46c3-82a4-8856cf065b01"
      },
      "outputs": [],
      "source": [
        "#mentions of israel v palestine in articles over time- output a csv\n",
        "# for each article -\n",
        "# make dict w each week, 0 counts of israel, pal, gaza, hamas\n",
        "# categorize into week\n",
        "# find counts, for each token increment category as appropriate\n",
        "df = pd.read_csv('./data/summary_20240421_articles.csv')\n",
        "df['date'] = pd.to_datetime(df['date'], format='mixed')\n",
        "counts_dict = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "root_directory = './'\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "\n",
        "\n",
        "    results_file = row['results_file']\n",
        "    article_file = row['article_file']\n",
        "\n",
        "    # Open NLP-analyzed result\n",
        "    filename = root_directory + results_file\n",
        "    try:\n",
        "        with open(filename) as d:\n",
        "            data = json.load(d)\n",
        "    except FileNotFoundError:\n",
        "        print('FILE NOT FOUND')\n",
        "        continue\n",
        "\n",
        "    # Open original text block from preprocessed data file\n",
        "    original_filename = root_directory + article_file\n",
        "    f = open(original_filename, \"r\")\n",
        "    article_text = f.read()\n",
        "    f.close()\n",
        "\n",
        "    # Extract original date\n",
        "    week_number = row['date'].date().isocalendar()[1]\n",
        "\n",
        "    # Extract NLP results\n",
        "    sentences = data[\"sentences\"]\n",
        "    text_all_sentences = extract_sentences(sentences)\n",
        "\n",
        "    # for sentence in text_all_sentences:\n",
        "    #     if any(word in sentence for word in PALESTINE_IDENTIFIERS):\n",
        "    #         counts_dict[week_number][\"palestine\"] += 1\n",
        "    #     if any(word in sentence for word in ISRAEL_IDENTIFIERS):\n",
        "    #         counts_dict[week_number][\"israel\"] += 1\n",
        "    #     if any(word in sentence for word in [\"Hamas\"]):\n",
        "    #         counts_dict[week_number][\"hamas\"] += 1\n",
        "\n",
        "    for sentence in text_all_sentences:\n",
        "        if any(word in sentence for word in pal_affiliates):\n",
        "            counts_dict[week_number][\"palestine\"] += 1\n",
        "        if any(word in sentence for word in ISRAEL_MEMBER_AFFILIATIONS):\n",
        "            counts_dict[week_number][\"israel\"] += 1\n",
        "\n",
        "df = pd.DataFrame(counts_dict).transpose()\n",
        "df.fillna(0, inplace=True)\n",
        "df.to_csv(root_directory + 'keyword_counts_incl_affiliates.csv', index=False)\n",
        "print(counts_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04bfb57e-f479-4551-a802-38d987ef48e7",
      "metadata": {
        "id": "04bfb57e-f479-4551-a802-38d987ef48e7"
      },
      "outputs": [],
      "source": [
        "!{sys.executable} -m pip install openai\n",
        "import openai"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}